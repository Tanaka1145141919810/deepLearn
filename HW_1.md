## 大作业
### 一个简单的深度学习系统
### 姓名：刘浩男
### 学号：24011211245

### 引入
为了更好的理解什么为什么深度学习可以正常的工作，所以制作了一个简单的可以进行深度学习的框架，当然，其十分的简陋，速度很慢，但是可以很好的了解深度学习的每个环节，以及他们深度学习每个环节在干什么。

### 自动求导的计算方法
在深度学习的网络训练的过程当中，我们总是需要一步一步的进行正向传播和反向传播，并且在这个过程之中不可避免的涉及到一次又一次的求导的过程，所以弄清楚自动求导的过程是非常有必要的。
涉及到求导的过程，感觉可以分成如下的几种常见的求导方法。
1. 手动根据数学知识写出求导的公式，然后将其转化为计算机实现的函数实现。
2. 利用导数的原始定义，通过有限差分近似方法完成求导，直接求解微分值。
3. 基于数学规则和程序表达式变换完成求导。利用求导规则对表达式进行自动计算，其计算结果是导函数的表达式而非具体的数值。即，先求解析解，然后转换为程序，再通过程序计算出函数的梯度.
4. 结合上溯几个方法的融合方法。

首先第一种方法是手动纸上计算的，不适合计算机的计算。
第二种方法是数值计算，其核心的公式是： $$df(x) = lim_{h->0}\frac{f(x + h) - f(x)}{h} $$
显然，我们可以通过如下的一个简单的Python程序来查看其效果
```python
def num_df(x : float , h : float , f: Callable[[float],float]):
    return (f(x+h) - f(x)) /h
def test_num_df(h : float)->list[float]:
    squre_def = lambda x : x**3
    arrlen = 20
    x_arr = [i for i in range(arrlen)]
    fx_arr = [squre_def(x) for x in x_arr]
    x_h_arr = [x+h for x in x_arr ]
    fx_h_arr = [squre_def(x_h) for x_h in x_h_arr]
    dx = []
    for i in range(arrlen):
        dx.append((fx_h_arr[i] - fx_arr[i]) / h)
    return dx
```
通过运行这个程序可以得到其结果和理论计算的结果有偏差，根据理论上的分析可以知道其结果为 $$dx =3x^{2}+ 3hx + h^{2} $$ 和标准的 $3x^{2} $ 有误差，并且误差和h有关。增大h的精度可以求出更好的结果，但是增加精度会扩大h的数据位宽，增大占用的内存，减缓执行的速度。这是想到了方法3,对于简单可以很好求导的函数，可以利用先验知识简单的求出其导数，无需数值求解。常见的导数如下:
$$d(x^{n}) = nx^{n-1}dx$$
$$d(ln(x)) = \frac{1}{x}dx $$
这样以来两种结合使用便可以很好的解决求导的误差问题。

### 自动求导的计算模式
对于神经网络的计算而言，可以分成简单的前向模式或者前向累积梯度和反向模式或者说反向累计梯度。下面以 $$f(x_{1},x_{2}) = lnx_{1} + x_{1}x_{2} - sin(x_{2}) $$这个函数为例子，首先画出其计算图。
<img src = "./resource/计算图1.svg">
根据简单的求导法则可以得到
$$\frac{dw}{dx_{1}} = \frac{dv_{1}}{dx_{1}} (\frac{dv_{2}}{dv_{1}}  \frac{dv_{3}}{dv_{2}} + \frac{dv_{5}}{dv_{1}}\frac{dv_{3}}{dv_{5}})\frac{dw}{dv_{3}} $$

首先从前向的梯度计算开始，即计算图从左向右计算梯度，下面举一个简单的例子，以简单的输入 $(x_{1},x_{2}) = (2,5) $ 为例子，首先是简单的节点的数值计算。

1. 首先是赋值 $v_{1} = x_{1} = 2$ ,$ v_{4} = x_{2} = 5$
2. 其次计算 $v_{2} =ln(v_{1}) = ln2 $
3. 其次计算 $v_{5} = v_{1}v_{4} = 10 $
4. 其次计算 $v_{3} = v_{2} + v_{5} = 10 + ln2 $
5. 其次计算 $v_{6} = sin(v_{4}) = sin5 $
6. 最后计算 $w = v_{3} - v_{6} = 10 + ln2 - sin5$

然后是对应的前向梯度的计算，其过程如下:

1. 由于 $v_{1} = x_{1} $ 所以可以得到 $\frac{dv_{1}}{dx_{1}} = 1 $
2. 由于 $v_{4} = x_{2} $ 所以可以得到 $\frac{dv_{4}}{dx_{1}} = 0 $
3. 计算 $v_{2} = ln(v_{1}) = ln(x_{1}) $ 所以可以得到 $\frac{dv_{2}}{dx_{1}} = \frac{1}{2} $
4. 计算 $dv_{5} = \frac{dv_{1}}{dx_{1}}v_{4} + v_{1}\frac{dv_{4}}{dx_{1}} = 5dx_{1}$
5. 计算 $dv_{6} = 0dx_{1} $
6. 最后计算 $\frac{dw}{dx_{1}} = 5.5 $

通过上面的这个例子可以看出前向梯度计算有如下的优缺点:
+ 实现起来很简单；
+ 也不需要很多额外的内存空间。
+ 每次前向计算只能计算对一个自变量的偏导数，对于一元函数求导是高效的，但是机器学习模型的自参数（入参）数量级大。
+ 如果有一个函数，其输入有 n 个，输出有 m 个，对于每个输入来说，前向模式都需要遍历计算过程以得到当前输入的导数，求解整个函数梯度需要 n 遍如上计算过程。

由于前向梯度计算有上述的优缺点，所以可见下面的反向求导数的方法：
反向模式根据从后向前计算，依次得到对每个中间变量节点的偏导数，直到到达自变量节点处，这样就得到了每个输入的偏导数。在每个节点处，根据该节点的后续节点（前向传播中的后续节点）计算其导数值。
就上面的计算图来说，从 $w$ 到 $x_{1}$ 可以明显的分成两条路径:
+ $w->v_{3}->v_{2}->v_{1}->x_{1} $
+ $w->v_{3}->v_{5}->v_{1}->x_{1} $

通过两条路径求导可以得到第一条路径得到的导数数值为0.5,第二条得到的导数数值为5。综上，可以得到的数值为5.5 同理对于 $\frac{dw}{dx_{2}}$ 的导数也可以得到，显然和上面的前向求导相比，反向传播有如下的优势:
+ 通过一次反向传输，就计算出所有偏导数，中间的偏导数计算只需计算一次。
+ 减少了重复计算的工作量，在多参数的时候后向自动微分的时间复杂度更低。
+ 需要额外的数据结构记录正向过程的计算操作，用于反向使用；
+ 带来了大量内存占用，为了减少内存操作，需要 AI 框架进行各种优化，也带来了额外限制和副作用。




